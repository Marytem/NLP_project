{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "#config.gpu_options.allow_growth=True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"correct_data1.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(591850, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 29288558\n",
      "total chars: 27\n"
     ]
    }
   ],
   "source": [
    "text = \"\\n\".join(df[1])\n",
    "print('corpus length:', len(text))\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))# cutting the SMILES corpus into the sequences used for training\n",
    "maxlen = 80\n",
    "step = 1\n",
    "text_train = \"\\n\".join(df[1][200000:400000])\n",
    "text_test = \"\\n\".join(df[1][400000:420000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 10386018\n",
      "Vectorization...\n",
      "Vectorization done\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text_train) - maxlen, step):\n",
    "    sentences.append(text_train[i: i + maxlen])\n",
    "    next_chars.append(text_train[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "print('Vectorization done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 1298978\n",
      "Vectorization...\n",
      "Vectorization done\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text_test) - maxlen, step):\n",
    "    sentences.append(text_test[i: i + maxlen])\n",
    "    next_chars.append(text_test[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print('Vectorization...')\n",
    "X_test = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y_test = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X_test[i, t, char_indices[char]] = 1\n",
    "    y_test[i, char_indices[next_chars[i]]] = 1\n",
    "print('Vectorization done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10386018, 80, 27)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10386018, 27)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, BatchNormalization, CuDNNLSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([CuDNNLSTM(256, input_shape=(X.shape[1],X.shape[2]),return_sequences=True),\n",
    "                   BatchNormalization(axis=1),\n",
    "                   CuDNNLSTM(256,return_sequences=False),\n",
    "                   BatchNormalization(axis=1),\n",
    "                   Dense(y.shape[1]),\n",
    "                   Activation('softmax')])\n",
    "# model.add()\n",
    "# model.add()\n",
    "# model.add(_\n",
    "# model.add()\n",
    "# model.add()\n",
    "# model.add()\n",
    "\n",
    "filename = \"checkpoints/cudnn-overfitting-bigdata-weights-improvement-02-0.5536.hdf5\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "                  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"checkpoints/cudnn-overfitting-bigdata-weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='loss', verbose=1, patience=7)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10386018/10386018 [==============================] - 2772s 267us/step - loss: 0.5342 - acc: 0.8116\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.53422, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-01-0.5342.hdf5\n",
      "Epoch 2/50\n",
      "10386018/10386018 [==============================] - 2839s 273us/step - loss: 0.5203 - acc: 0.8164\n",
      "\n",
      "Epoch 00002: loss improved from 0.53422 to 0.52033, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-02-0.5203.hdf5\n",
      "Epoch 3/50\n",
      "10386018/10386018 [==============================] - 2721s 262us/step - loss: 0.5096 - acc: 0.8202\n",
      "\n",
      "Epoch 00003: loss improved from 0.52033 to 0.50959, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-03-0.5096.hdf5\n",
      "Epoch 4/50\n",
      "10386018/10386018 [==============================] - 2850s 274us/step - loss: 0.5011 - acc: 0.8232\n",
      "\n",
      "Epoch 00004: loss improved from 0.50959 to 0.50107, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-04-0.5011.hdf5\n",
      "Epoch 5/50\n",
      "10386018/10386018 [==============================] - 2782s 268us/step - loss: 0.4939 - acc: 0.8257\n",
      "\n",
      "Epoch 00005: loss improved from 0.50107 to 0.49389, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-05-0.4939.hdf5\n",
      "Epoch 6/50\n",
      "10386018/10386018 [==============================] - 2824s 272us/step - loss: 0.4881 - acc: 0.8278\n",
      "\n",
      "Epoch 00006: loss improved from 0.49389 to 0.48807, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-06-0.4881.hdf5\n",
      "Epoch 7/50\n",
      "10386018/10386018 [==============================] - 2937s 283us/step - loss: 0.4829 - acc: 0.8295\n",
      "\n",
      "Epoch 00007: loss improved from 0.48807 to 0.48291, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-07-0.4829.hdf5\n",
      "Epoch 8/50\n",
      "10386018/10386018 [==============================] - 2764s 266us/step - loss: 0.4784 - acc: 0.8310\n",
      "\n",
      "Epoch 00008: loss improved from 0.48291 to 0.47837, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-08-0.4784.hdf5\n",
      "Epoch 9/50\n",
      "10386018/10386018 [==============================] - 2824s 272us/step - loss: 0.4745 - acc: 0.8325\n",
      "\n",
      "Epoch 00009: loss improved from 0.47837 to 0.47446, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-09-0.4745.hdf5\n",
      "Epoch 10/50\n",
      "10386018/10386018 [==============================] - 2803s 270us/step - loss: 0.4709 - acc: 0.8336\n",
      "\n",
      "Epoch 00010: loss improved from 0.47446 to 0.47092, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-10-0.4709.hdf5\n",
      "Epoch 11/50\n",
      "10386018/10386018 [==============================] - 2786s 268us/step - loss: 0.4679 - acc: 0.8345\n",
      "\n",
      "Epoch 00011: loss improved from 0.47092 to 0.46791, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-11-0.4679.hdf5\n",
      "Epoch 12/50\n",
      "10386018/10386018 [==============================] - 2839s 273us/step - loss: 0.4649 - acc: 0.8356\n",
      "\n",
      "Epoch 00012: loss improved from 0.46791 to 0.46486, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-12-0.4649.hdf5\n",
      "Epoch 13/50\n",
      "10386018/10386018 [==============================] - 2790s 269us/step - loss: 0.4625 - acc: 0.8365\n",
      "\n",
      "Epoch 00013: loss improved from 0.46486 to 0.46246, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-13-0.4625.hdf5\n",
      "Epoch 14/50\n",
      "10386018/10386018 [==============================] - 2789s 269us/step - loss: 0.4599 - acc: 0.8372\n",
      "\n",
      "Epoch 00014: loss improved from 0.46246 to 0.45992, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-14-0.4599.hdf5\n",
      "Epoch 15/50\n",
      "10386018/10386018 [==============================] - 2812s 271us/step - loss: 0.4576 - acc: 0.8380\n",
      "\n",
      "Epoch 00015: loss improved from 0.45992 to 0.45763, saving model to checkpoints/cudnn-overfitting-bigdata-weights-improvement-15-0.4576.hdf5\n",
      "Epoch 16/50\n",
      "  521216/10386018 [>.............................] - ETA: 42:25 - loss: 0.4433 - acc: 0.8427"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dcc40f1422bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    183\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 185\u001b[0;31m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l_history = model.fit(X, y, epochs=50, batch_size=1024, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(l_history.history).to_csv(\"history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sed = np.zeros(maxlen, len(chars)), dtype=np.bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" 3(F)C(O)CC12C)OC(=O)C\n",
      "Cc1oc(C=C2SC(=O)N(Cc3ccc4ccccc4c3)C2=O)cc1\n",
      "CC1Cc2c(OCc3ccc \"\n",
      "cc3)c(C)nc4c(C=CC(=O)NCCO)sc2c5CCCC15\n",
      "CC(C)CC(NC(=O)C(Cc1ccccc1)NC(=O)C(Cc2ccccc2)NC(=O)C(CCC(=O)N)NC(=O)C(CC(C)C)NC(=O)C(CCC(=O)N)NC(=O)C3CCCN3C(=O)C(CCCCN)NC(=O)C4CCCN4C(=O)C(N)CCCN=C(N)N)C(=O)N\n",
      "CC(C)CC(NC(=O)C(CC(C)C)NC(=O)C(CCC(=O)O)NC(=O)C(CC(C)C)NC(=O)C(C)NC(=O)C(C)NC(=O)C(CCC(=O)O)NC(=O)C(CC(C)C)NC(=O)C(CCC(=O)O)NC(=O)C(CC(C)C)NC(=O)C(C)NC(=O)C(C)NC(=O)C(CC(C)C)NC(=O)C(CCC(=O)O)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(CC(C\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(X_test)-1)\n",
    "pattern = X_test[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([indices_char[value.argmax()] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern,(1,pattern.shape[0],pattern.shape[1]))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = indices_char[index]\n",
    "    seq_in = [indices_char[value.argmax()]for value in pattern]\n",
    "    print(result,end=\"\")\n",
    "    row = np.zeros(len(chars))\n",
    "    row[index] = 1\n",
    "    pattern = np.vstack([pattern,row])\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1298978/1298978 [==============================] - 66s 51us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5135894857326696, 0.8297646303400575]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_accuracy_curves(history, title):\n",
    "  \n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plt_loss_curves(history, title):\n",
    "  \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_accuracy_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_loss_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"C(CCCNC(=N)N)NC(=O)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"CCCCCCCCCCCCCCCCNc1ccc(cc1)C(=O)OCC(O)CO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
